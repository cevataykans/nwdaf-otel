apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: udm-latency-scale
  namespace: aether-5gc
spec:
  scaleTargetRef:
    kind: Deployment
    name: udm
  pollingInterval: 1       # how often KEDA polls info, maybe overridden by IsActiveSeries gRPC call in external scaler.
  cooldownPeriod: 120      # wait time after scale-down conditions are met
  minReplicaCount: 1
  maxReplicaCount: 10
  advanced:                                          # Optional. Section to specify advanced options
    restoreToOriginalReplicaCount: true/false        # Optional. Default: false
    horizontalPodAutoscalerConfig:                   # Optional. Section to specify HPA related options
#      name: { name-of-hpa-resource }                 # Optional. Default: keda-hpa-{scaled-object-name}
      behavior:                                      # Optional. Use to modify HPA's scaling behavior
        scaleUp:
          stabilizationWindowSeconds: 5   # confirm latency really stays high
          policies:
            - type: Pods
              value: 1
              periodSeconds: 5
          selectPolicy: Max

  # https://keda.sh/docs/2.18/concepts/external-scalers/
  triggers:
    - type: external
      metadata:
        scalerAddress: external-scaler.aether-5gc.svc.cluster.local:8081
#        longitude: "-122.335167"
#        latitude: "47.608013"

#    - type: prometheus
#      metadata:
#        serverAddress: http://prometheus-operated.cattle-monitoring-system.svc.cluster.local:9090
#        # ðŸ‘‡ Replace this PromQL with your real latency metric
#        query: |
#          histogram_quantile(0.95,
#            sum by(le) (
#              rate(
#                traces_spanmetrics_latency_bucket{service_name=~"udm.aether-5gc", span_name="POST /nudm-ueau/v1/{var}/auth-events"}[1m]
#              )
#            )
#          )
#        threshold: "4.00"  # scale if p95 latency ("0.25" ) > 250ms, "4.00" -> 4 seconds