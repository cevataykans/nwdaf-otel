apiVersion: opentelemetry.io/v1beta1
kind: OpenTelemetryCollector
metadata:
  name: simplest
# Annotations for PROM is disabled, because ServiceMonitor is introduced for changing Tracing labels.
#  annotations:
#    prometheus.io/scrape: "true"
#    prometheus.io/port: "9089"
spec:
  managementState: managed
  image: "ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-contrib:0.136.0"
  ports:
    - name: trace-metrics # For prometheus exporter port
      port: 9089
  config:
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318
      # For collecting own metrics
      # as this is not used in a pipeline, is ignored during startup
#      prometheus/own:
#        config:
#          scrape_configs:
#            - job_name: 'otel-collector'
#              scrape_interval: 5s
#              static_configs:
#                - targets: ['otel-collector:8888']

    # connectors to generate metrics from the incoming spans
    connectors:
      # basic service graph
      servicegraph:
        latency_histogram_buckets: [100ms, 250ms, 1s, 5s, 10s]
      # a bit more elaborate spanmetrics
      spanmetrics:
        # this is relevant, since grafana expects the traces_spanmetrics_ prefix
        namespace: traces.spanmetrics
        # this is relevant, since grafana expects the historgram metrics in seconds
        histogram:
          unit: "s"
        # the rest is close to default with some minor QoL additions
        dimensions:
          - name: http.method
            default: GET
          - name: http.status_code
        exemplars:
          enabled: true
        events:
          enabled: true
          dimensions:
            - name: exception.type
            - name: exception.message
        resource_metrics_key_attributes:
          - service.name
          - telemetry.sdk.language
          - telemetry.sdk.name
    processors:
      batch:
        timeout: 1s            # Adjust the timeout to allow more time for processing
        send_batch_size: 4096    # Set a higher send batch size to handle more traces at once
      attributes/trace:
        actions:
          - key: response_flags
            action: delete
          - key: upstream_cluster
            action: delete
          - key: downstream_cluster
            action: delete
          - key: http.protocol
            action: delete
          - key: istio.canonical_revision
            action: delete
          - key: istio.mesh_id
            action: delete
          - key: node_id
            action: delete
          - key: upstream_cluster.name
            action: delete
          - key: user_agent
            action: delete
          - key: zone
            action: delete
          - key: istio.namespace
            action: delete
          - key: guid:x-request-id
            action: delete
          - key: component
            action: delete

      filter/traces:
        spans:
          exclude:
            match_type: strict
            attributes:
              #- key: grpc.authority
              # value: "webui:9876"
              #- key: k8s.pod.service_name
              #  value: "webui.aether-5gc"
              - key: istio.canonical_service
                value: "simapp"

              #error_mode: ignore
              #traces:
              #span:
              #- IsMatch(resource.attributes["k8s.pod.name"], "my-pod-name.*")
      memory_limiter:
        limit_mib: 12000  # Increase memory limit
        check_interval: 1s
        spike_limit_mib: 2048
        #  batch:
        #send_batch_size: 1000
        #timeout: 5s
        #queued_retry:
        #num_workers: 4
        #queue_size: 10000  # Increase queue size
    exporters:
      # Expose an endpoint for aether roc so prometheus can pull data periodically
      prometheus/aether:
        endpoint: "0.0.0.0:9089" #"http://prometheus.monitoring.svc.cluster.local:9090"
          #tls:
      otlp/jaegar:
        endpoint: "http://jaeger-collector.default.svc.cluster.local:4317"
        tls:
          insecure: true
        retry_on_failure:
          enabled: true
          initial_interval: 5s  # Delay before first retry
          max_interval: 30s     # Maximum delay between retries
          max_elapsed_time: 300s # Maximum total retry duration
        sending_queue:
          enabled: true
          num_consumers: 4       # Number of workers retrying spans
          queue_size: 10000      # Large buffer to store spans before dropping
      otlp/tempo:
        endpoint: "http://tempo.default.svc.cluster.local:4317"
        tls:
          insecure: true

    service:
      # values from: https://opentelemetry.io/docs/collector/internal-telemetry/#activate-internal-telemetry-in-the-collector
      telemetry:
        metrics:
          level: normal #detailed
#          readers:
#            - pull:
#                exporter:
#                  prometheus:
#                    host: '0.0.0.0'
#                    port: 8888
# default log level is already info
#        logs:
#          level: info #debug
      pipelines:
        traces:
          receivers: [otlp]
          processors: [batch, filter/traces, attributes/trace , memory_limiter]    # filter/traces] #, queued_retry] #, resource]
          exporters: [otlp/jaegar, otlp/tempo, servicegraph, spanmetrics]
        metrics:
          receivers: [otlp, servicegraph, spanmetrics] # prometheus scrapes its own metrics
          processors: [batch]
          exporters: [prometheus/aether]